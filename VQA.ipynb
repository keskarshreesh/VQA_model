{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of nnprojectffinal.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "le2-o5eOVSzO",
        "colab_type": "code",
        "outputId": "4f7896df-48c3-4ae5-f6ad-110634ac8797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#Necessary if working on colab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-4RaUjX5WToN",
        "colab_type": "code",
        "outputId": "2b3043a9-fd4a-4353-a5b6-a8d442243f95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd gdrive/My Drive/Training\n",
        "\n",
        "#change working directory to where data is stored."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CKrMos2Zaete",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "K0blruR0W18w",
        "colab_type": "code",
        "outputId": "b901115c-6241-4347-8abf-9b3d7d5296d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "data = json.load(open('Quest_Answers.json')) #json file containing image names, corresponding questions and answers.\n",
        "\n",
        "# data[\"quest_answers\"]\n",
        "df = pd.DataFrame(data[\"quest_answers\"]) #Respresent as data frame\n",
        "print(df['Image'][0])\n",
        "print(df['Question'][0])\n",
        "print(df['Answer'][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CLEVR_new_000000\n",
            "There is a metal thing that is in front of the gray thing right of the big blue shiny sphere; how many rubber cubes are in front of it?\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rFRKVnDFAAQX",
        "colab_type": "code",
        "outputId": "49926f46-4ece-47f9-9246-89ee9966953d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "currpath = os.getcwd()\n",
        "print(glob.glob(currpath))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/gdrive/My Drive/Training']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "61NL0lUvAEMA",
        "colab_type": "code",
        "outputId": "45e68b51-1089-4aff-e005-93295ddf5f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "#images = [cv2.imread(file), print(images.length()) for file in glob.glob(currpath+\"/images/*.png\")]\n",
        "\n",
        "#df['pic'] = [cv2.imread(glob.glob(currpath)+\"/images/\"+file+\".png\") for file in df['Image']]\n",
        "\n",
        "images = []\n",
        "names = df['Image']\n",
        "names = list(sorted(set(names)))\n",
        "#bad = []\n",
        "len(names)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13503"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "hEBKzzM9E-qs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "imagedc = pickle.load(open(currpath + '/array64','rb')) #load image dictionary from pickle file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wRxhG6vgALL_",
        "colab_type": "code",
        "outputId": "743959cc-e5c4-43e1-969c-ac90acac6a79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "cell_type": "code",
      "source": [
        "#Creating image array\n",
        "images = []\n",
        "for i in range(len(names)): \n",
        "    p = str(glob.glob(currpath)[0])+\"/images/\"+str(names[i])+\".png\"\n",
        "    if i%100 == 0:\n",
        "      print(p)\n",
        "    img = cv2.imread(p)\n",
        "    if img is not None:\n",
        "      img = cv2.resize(img, (120,160))\n",
        "      images.append(img)\n",
        "      #print(file)\n",
        "    else:\n",
        "      #print(file)\n",
        "      break\n",
        "      \n",
        "      \n",
        "\n",
        "print(len(images)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Training/images/CLEVR_new_000000.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c7acc1ee6fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "r893uvYmASbm",
        "colab_type": "code",
        "outputId": "df432c48-bb7b-41bd-cead-8eb2176c559b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Dividing data into train and test sets, train_test_split can also be used.\n",
        "\n",
        "print(len(imagedc))\n",
        "import numpy as np\n",
        "x_train_im = []#np.empty(shape = (50000,120,160,3))\n",
        "x_train_q = [] #np.empty(shape = 50000, dtype = object)\n",
        "y = [] #np.empty(shape = 50000, dtype = \"S10\")\n",
        "cnt = 0\n",
        "previm = df['Image'][0]\n",
        "currim = df['Image'][0]\n",
        "for i in range(0,135020):\n",
        "    if(previm != currim):\n",
        "        cnt = cnt+1\n",
        "    if(df['Image'][cnt] in names):\n",
        "      x_train_im.append(imagedc[cnt])\n",
        "      x_train_q.append(df['Question'][i])\n",
        "      y.append(df['Answer'][i])\n",
        "    #x_train_im = np.append(x_train_im, images[cnt])\n",
        "    if(i < (len(df)-1)):\n",
        "        previm = currim\n",
        "        currim = df['Image'][i+1]\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13503\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "up-oTKTLXCO6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# imagefnames= []\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for i in range(0,len(df)):\n",
        "  imname = df['Image'][i] + \".png\"\n",
        "  question = df['Question'][i]\n",
        "  answer = df['Answer'][i]\n",
        "#   if imname not in imagedc:\n",
        "#     continue\n",
        "#   imagefnames.append(imname)\n",
        "  questions.append(question)\n",
        "  answers.append(answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jPRwa4P2XeN-",
        "colab_type": "code",
        "outputId": "577cb4fa-bd17-4d07-e525-d30772c613a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "answers = np.array(answers)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#One-hot encoding the answers\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(answers)\n",
        "# # print(integer_encoded)\n",
        "# # binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=True)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "\n",
        "Y = onehot_encoded \n",
        "\n",
        "#Saving label_encoder in pickle file.\n",
        "\n",
        "print(\"Saving label_encoder.pickle\")\n",
        "import pickle\n",
        "with open('label_encoder.pickle', 'wb') as handle:\n",
        "     pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "     print('label_encoder.pickle saved')\n",
        "\n",
        "print(onehot_encoded)\n",
        "# # invert first example\n",
        "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
        "print(inverted)\n",
        "label_encoder.classes_\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(char_level=False,lower=False)\n",
        "tokenizer.fit_on_texts(answers)\n",
        "sequence_of_int = tokenizer.texts_to_sequences(answers)\n",
        "# print(sequence_of_int)\n",
        "\n",
        "y_hot = keras.utils.to_categorical(sequence_of_int)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "\n",
        "# print(\"------------\", Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-903f2d2a3336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msequence_of_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# print(sequence_of_int)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    220\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                                             self.split)\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtranslate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mtranslate_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'translate'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Nop7EXPX9YqC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(len(y_hot))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tpEJIn4rd_Gg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# imagefnames = np.array(imagefnames)\n",
        "questions = np.array(questions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7VWu9zIiZCqg",
        "colab_type": "code",
        "outputId": "595598f5-8331-492c-9059-9e290d647efd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "#Tokenizing question data\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "n_most_common_words = 80\n",
        "max_len = 30\n",
        "tokenizer = Tokenizer(num_words=n_most_common_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(questions)\n",
        "sequences = tokenizer.texts_to_sequences(questions)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "X_q = pad_sequences(sequences, maxlen=max_len)\n",
        "print(X_q.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 1, 'is': 2, 'of': 3, 'that': 4, 'same': 5, 'thing': 6, 'object': 7, 'what': 8, 'as': 9, 'there': 10, 'are': 11, 'cylinder': 12, 'big': 13, 'rubber': 14, 'large': 15, 'tiny': 16, 'matte': 17, 'small': 18, 'a': 19, 'number': 20, 'color': 21, 'shape': 22, 'left': 23, 'behind': 24, 'right': 25, 'in': 26, 'front': 27, 'things': 28, 'metallic': 29, 'size': 30, 'metal': 31, 'shiny': 32, 'material': 33, 'objects': 34, 'brown': 35, 'red': 36, 'purple': 37, 'cyan': 38, 'green': 39, 'gray': 40, 'yellow': 41, 'blue': 42, 'sphere': 43, 'ball': 44, 'cube': 45, 'block': 46, 'it': 47, 'to': 48, 'on': 49, 'side': 50, 'how': 51, 'have': 52, 'many': 53, 'and': 54, 'other': 55, 'cylinders': 56, 'or': 57, 'any': 58, 'made': 59, 'its': 60, 'than': 61, 'does': 62, 'cubes': 63, 'spheres': 64, 'balls': 65, 'blocks': 66, 'either': 67, 'has': 68, 'do': 69, 'both': 70, 'fewer': 71, 'greater': 72, 'less': 73, 'more': 74, 'another': 75, 'anything': 76, 'else': 77, 'an': 78, 'equal': 79, 'visible': 80}\n",
            "Found 80 unique tokens.\n",
            "(135020, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fkquoICEZJJs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#image data generator\n",
        "\n",
        "def generator(image_dict, img_names, questions, labels, batch_size):\n",
        "    \n",
        "    q_ptr = 0\n",
        "    while True:\n",
        "        image_inp = []\n",
        "        q_inp = []\n",
        "        batch_labels = []\n",
        "        for i in range(batch_size):\n",
        "            if q_ptr == len(questions):\n",
        "                q_ptr = 0\n",
        "            index = q_ptr\n",
        "#             import random\n",
        "#             index= random.randint(0, len(questions)-1)\n",
        "            # print(imageNamesX[q_ptr].shape)\n",
        "            # print(questionsX[q_ptr])\n",
        "            image = image_dict[img_names[index]]\n",
        "            image = image*(1./255)\n",
        "            image_inp.append(image)\n",
        "            q_inp.append(questions[index])\n",
        "            batch_labels.append(labels[index])\n",
        "            q_ptr+=1\n",
        "            \n",
        "        yield [np.array(image_inp), np.array(q_inp)], np.array(batch_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jnS6PnukYY9y",
        "colab_type": "code",
        "outputId": "c5a2ecc9-5492-45dc-9df2-935a027481c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "imnames_train, imnames_test, questionsX_train, questionsX_test, Y_train, Y_test = train_test_split(imagefnames, X_q, y_hot, test_size=0.1, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4eeb43b92c12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimnames_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimnames_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestionsX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestionsX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagefnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'imagefnames' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "OB_bzR2JZgTY",
        "colab_type": "code",
        "outputId": "41615253-f296-4e47-df25-6d41024696e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#Final Model, changed from original model provided by Keras\n",
        "\n",
        "import keras\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "from keras.layers import Input, LSTM, Embedding, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "# Define CNN for Image Input\n",
        "vision_model = Sequential()\n",
        "vision_model.add(Conv2D(32, (3, 3), activation='selu', padding='same', input_shape=(64, 64, 3)))\n",
        "vision_model.add(Conv2D(32, (3, 3), activation='selu'))\n",
        "vision_model.add(MaxPooling2D((2, 2)))\n",
        "vision_model.add(Conv2D(64, (3, 3), activation='selu', padding='same'))\n",
        "vision_model.add(Conv2D(64, (3, 3), activation='selu'))\n",
        "vision_model.add(MaxPooling2D((2, 2)))\n",
        "vision_model.add(Conv2D(128, (3, 3), activation='selu', padding='same'))\n",
        "vision_model.add(Conv2D(128, (3, 3), activation='selu'))\n",
        "vision_model.add(Conv2D(128, (3, 3), activation='selu'))\n",
        "vision_model.add(MaxPooling2D((2, 2)))\n",
        "vision_model.add(Flatten())\n",
        "\n",
        "image_input = Input(shape=(64, 64, 3))\n",
        "print(image_input)\n",
        "# image_input = images\n",
        "encoded_image = vision_model(image_input)\n",
        "\n",
        "# Defined RNN for language input\n",
        "question_input = Input(shape=(30,), dtype='int32')\n",
        "embedded_question = Embedding(input_dim=81 ,output_dim=128, input_length=30)(question_input)\n",
        "encoded_question = LSTM(128)(embedded_question)\n",
        "\n",
        "# Combined CNN and RNN to create the final model\n",
        "merged = keras.layers.concatenate([encoded_question, encoded_image])\n",
        "d1  = Dense(512, activation='sigmoid')(merged)\n",
        "dp1 = Dropout(0.5)(d1)\n",
        "output = Dense(27, activation='softmax')(dp1)\n",
        "\n",
        "vqa_model = Model(inputs=[image_input, question_input], outputs=output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"input_1:0\", shape=(?, 64, 64, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3oXxBFnPZq2K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vqa_model.compile(optimizer=keras.optimizers.Adam(epsilon=0.01,lr = 0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ibnPkAgKfU1r",
        "colab_type": "code",
        "outputId": "95cbe139-0074-4917-b462-865f8b5b7e2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "val_steps = len(Y_test)/(batch_size-1)\n",
        "train_steps = len(Y_train)/(batch_size-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0ea971cec9e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mval_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Y_test' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "952jGRZ9aGjZ",
        "colab_type": "code",
        "outputId": "31ef6a97-ecd0-4749-c111-1f6117ded520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "cell_type": "code",
      "source": [
        "# vqa_model.fit_generator(generator(imagedc, imnames_train, questionsX_train, Y_train, batch_size),validation_data=generator(imagedc, imnames_test, questionsX_test, Y_test, batch_size),validation_steps=val_steps, steps_per_epoch=train_steps, epochs=1)\n",
        "history = vqa_model.fit(x = [x_train_im[0:100000], X_q[0:100000]], y = y_hot[0:100000], epochs=4, batch_size=128, verbose=1, validation_split = 0.2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-26b57eb8c496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvqa_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_train_im\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y_hot' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "BsTnItuma05n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vqa_model.save('litemodel3.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2zl9MJj5D95o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "matrix = vqa_model.predict(x = [x_train_im[80000: 80050], X_q[80000:80050]])\n",
        "# vqa_model.predict(imagedc, imnames_test,questionsX_test)\n",
        "np.savetxt(\"fileBaba20Percent_1\", matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lAWvTKUc5bkZ",
        "colab_type": "code",
        "outputId": "4a536de1-ab04-4ac1-a201-4294f5a833d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "modeL = load_model(currpath + \"/litemodel4.h5\")\n",
        "\n",
        "score,acc = modeL.evaluate(x = [x_train_im[0:100000], X_q[0:100000]], y = Y[0:100000], batch_size = 256)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-30c1b8430e45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodeL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/litemodel4.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodeL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_train_im\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (120, 160, 3) but got array with shape (64, 64, 3)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "fd_dMY_MEYMP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}